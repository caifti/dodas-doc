{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Dynamic On Demand Analysis Service: DODAS Dynamic On Demand Analysis Service (DODAS) is a Platform as a Service tool built combining several solutions and products developed by the INDIGO-DataCloud H2020 project and now part of the EOSC-hub H2020 Project. DODAS allows to instantiate on-demand container-based clusters (Apache Mesos) over any cloud with almost zero effort and with very limited knowledge of the underlying technical details DODAS provides the end user with all the support to deploy both HTCondor batch system and platforms for the Big Data analysis based on Spark, Hadoop etc. DODAS, as a Thematic Services in the context of EOSC-hub project, is financially supported by European Union\u2019s Horizon 2020 research and innovation programme, grant agreement RIA 777536.","title":"Dynamic On Demand Analysis Service: DODAS"},{"location":"#dynamic-on-demand-analysis-service-dodas","text":"Dynamic On Demand Analysis Service (DODAS) is a Platform as a Service tool built combining several solutions and products developed by the INDIGO-DataCloud H2020 project and now part of the EOSC-hub H2020 Project. DODAS allows to instantiate on-demand container-based clusters (Apache Mesos) over any cloud with almost zero effort and with very limited knowledge of the underlying technical details DODAS provides the end user with all the support to deploy both HTCondor batch system and platforms for the Big Data analysis based on Spark, Hadoop etc. DODAS, as a Thematic Services in the context of EOSC-hub project, is financially supported by European Union\u2019s Horizon 2020 research and innovation programme, grant agreement RIA 777536.","title":"Dynamic On Demand Analysis Service: DODAS"},{"location":"contacts/","text":"Contact us DODAS Team provides two support channels, email and Slack channel. For any question you might have send a message to the following list: dodas-support_AT_lists.infn.it first. Slack, if suitable for everybody, will be then activated.","title":"Contact us"},{"location":"contacts/#contact-us","text":"DODAS Team provides two support channels, email and Slack channel. For any question you might have send a message to the following list: dodas-support_AT_lists.infn.it first. Slack, if suitable for everybody, will be then activated.","title":"Contact us"},{"location":"dodas-how-it-is-made/","text":"DODAS: How it is made DODAS Architecture: Basic Concepts DODAS has a highly modular architecture and the workflows are highly customisable. This is extremely valuable for a user because it is a key to the possibility of extending the already provided configuration with anything the user might need (from software dependencies up to the integration of external services passing through the user tailored code management) . This kind of flexibility is achieved thanks to the modularity of the overall system which is shortly described below. The major sub-services composing DODAS are: the INDIGO Identity and Access Management (IAM) and Token Translation Service (TTS), PaaS Orchestrator and Infrastructure Manager (IM). Those composition of services represents the so-called PaaS Core Services of DODAS. IAM is the OpenidConnect Authorization Server which has the crucial role to authenticate the user and to provide her/him with a OIDC token which in turn is used to delegate service to act on behalf of the user itself. While the user must register to the IAM-DODAS instance, a federated system supporting eduGAIN, the PaaS Orchestrator and/or IM are responsible to take care of the user request (in form of a TOSCA template) and prepare a cluster for containers orchestration over the IaaS . DODAS relies on Mesos for the container orchestration. The container orchestrator is meant as a layer where the end-users service can be executed. Regarding the PaaS Orchestrator and IM, both have the role of abstracting the underlying IaaSes and both of them support the same TOSCA language for input data description. There are differences among the two, and there are additional features (with respect to the abstraction) which are only supported by the PaaS Orchestrator. Concrete examples are: A user needs to access a multi cloud environment, which in principle could be a combination of both public and private provider (hybrid model), in this case the Orchestrator guarantee a transparent management of the underlying IaaSes A user needs support to the elasticity meant as an elastic extension of cluster based on load, the PaaS Orchestrator provides the proper support thanks to the integration with Clues . On the other hand it is worth mentioning that using your own public or private cloud through the orchestrator requires some registration steps , while the IM does not require any resource registration. A high level view from the end user perspectives DODAS from the user perspective is a service aiming at enabling an easy solution for the creation of a complex setup for a computational environment on any cloud based environment. In other words, DODAS aim is to make the process of generating intricate setups as easy as it is today creating a virtual machine on any IaaS: a one-click solution. The summary of the major added values of DODAS for a scientist is: To provide a complete abstraction of the underlying clouds To automate the virtual hardware provisioning and configuration To provide a cluster platform with a high level of self-healing To guarantee setup and service customization to cope with specific requirements More concretely \"a complex setup\" in this context means a container orchestrator (e.g. Mesos) on top of which there could be any framework which in turn manages the user service. The user service can be anything in principle, however DODAS provides two principal baselines ready to be used and to be possibly extended: a HTCondor batch system and a Spark cluster. Dealing with DODAS for a user means to configure and submit a TOSCA template. Several templates have already been developed, one per supported use case (see this section for further details) but, of course, it is worth to remark that the key value is that you can either extend any of them or create your own. For the sake of completeness the very first step before developing and / or using existing templates is to register, and this must be done through the IAM-DODAS service. Except the registration step there is nothing else which represents a pre-requisite DODAS specific. There are, of course, pre-requisites both if you are about to use the CMS and AMS implementation of DODAS as well as if you are supposed to External IaaS (read as some cloud different from the provided Enabling Facility ), as explained here .","title":"DODAS: How it is made"},{"location":"dodas-how-it-is-made/#dodas-how-it-is-made","text":"","title":"DODAS: How it is made"},{"location":"dodas-how-it-is-made/#dodas-architecture-basic-concepts","text":"DODAS has a highly modular architecture and the workflows are highly customisable. This is extremely valuable for a user because it is a key to the possibility of extending the already provided configuration with anything the user might need (from software dependencies up to the integration of external services passing through the user tailored code management) . This kind of flexibility is achieved thanks to the modularity of the overall system which is shortly described below. The major sub-services composing DODAS are: the INDIGO Identity and Access Management (IAM) and Token Translation Service (TTS), PaaS Orchestrator and Infrastructure Manager (IM). Those composition of services represents the so-called PaaS Core Services of DODAS. IAM is the OpenidConnect Authorization Server which has the crucial role to authenticate the user and to provide her/him with a OIDC token which in turn is used to delegate service to act on behalf of the user itself. While the user must register to the IAM-DODAS instance, a federated system supporting eduGAIN, the PaaS Orchestrator and/or IM are responsible to take care of the user request (in form of a TOSCA template) and prepare a cluster for containers orchestration over the IaaS . DODAS relies on Mesos for the container orchestration. The container orchestrator is meant as a layer where the end-users service can be executed. Regarding the PaaS Orchestrator and IM, both have the role of abstracting the underlying IaaSes and both of them support the same TOSCA language for input data description. There are differences among the two, and there are additional features (with respect to the abstraction) which are only supported by the PaaS Orchestrator. Concrete examples are: A user needs to access a multi cloud environment, which in principle could be a combination of both public and private provider (hybrid model), in this case the Orchestrator guarantee a transparent management of the underlying IaaSes A user needs support to the elasticity meant as an elastic extension of cluster based on load, the PaaS Orchestrator provides the proper support thanks to the integration with Clues . On the other hand it is worth mentioning that using your own public or private cloud through the orchestrator requires some registration steps , while the IM does not require any resource registration.","title":"DODAS Architecture: Basic Concepts"},{"location":"dodas-how-it-is-made/#a-high-level-view-from-the-end-user-perspectives","text":"DODAS from the user perspective is a service aiming at enabling an easy solution for the creation of a complex setup for a computational environment on any cloud based environment. In other words, DODAS aim is to make the process of generating intricate setups as easy as it is today creating a virtual machine on any IaaS: a one-click solution. The summary of the major added values of DODAS for a scientist is: To provide a complete abstraction of the underlying clouds To automate the virtual hardware provisioning and configuration To provide a cluster platform with a high level of self-healing To guarantee setup and service customization to cope with specific requirements More concretely \"a complex setup\" in this context means a container orchestrator (e.g. Mesos) on top of which there could be any framework which in turn manages the user service. The user service can be anything in principle, however DODAS provides two principal baselines ready to be used and to be possibly extended: a HTCondor batch system and a Spark cluster. Dealing with DODAS for a user means to configure and submit a TOSCA template. Several templates have already been developed, one per supported use case (see this section for further details) but, of course, it is worth to remark that the key value is that you can either extend any of them or create your own. For the sake of completeness the very first step before developing and / or using existing templates is to register, and this must be done through the IAM-DODAS service. Except the registration step there is nothing else which represents a pre-requisite DODAS specific. There are, of course, pre-requisites both if you are about to use the CMS and AMS implementation of DODAS as well as if you are supposed to External IaaS (read as some cloud different from the provided Enabling Facility ), as explained here .","title":"A high level view from the end user perspectives"},{"location":"faq/","text":"FAQ Work in progress","title":"FAQ"},{"location":"faq/#faq","text":"Work in progress","title":"FAQ"},{"location":"introduction/","text":"Introduction The mission of DODAS is to act as a cloud enabler for scientists seeking to easily exploit distributed and heterogeneous clouds to process, manipulate or generate data. Aiming to reduce the learning curve, as well as the operational cost of managing community specific services running on distributed cloud, DODAS completely automates the process of provisioning, creating, managing and accessing a pool of heterogeneous computing and storage resources. Within the EOSC-hub project DODAS - Thematic Service is providing both the PaaS core services and an Enabling Facility (Cloud@CNAF and Cloud@ReCaS-Bari). Although DODAS PaaS core layer can be used to exploit any cloud , we foresee that a user might benefit of a freely accessible Enabling Facility where to test a customisation and/or simply try out how DODAS behaves etc. This guide provides not only an overview of the architecture and basic concepts behind DODAS, but also a comprehensive documentation for users willing to instantiate currently supported services: HTCondor batch system as a Service Big Data platform for ML as a Service either using the DODAS provided Enabling Facility or a user provided public Cloud as well as a private Cloud (Amazon, Microsoft Azure, Open Telekom Cloud, etc). Moreover this guide provides the reader guidelines on how to customise and extend the DODAS workflow. However, if you feel you are an impatient user, this is the section you are looking for. DODAS has been integrated by the Submission Infrastructure of Compact Muon Solenoid (CMS), one of the two bigger and general purposes experiments at LHC of CERN, as well as by the Alpha Magnetic Spectrometer (AMS-02) computing environment.","title":"Introduction"},{"location":"introduction/#introduction","text":"The mission of DODAS is to act as a cloud enabler for scientists seeking to easily exploit distributed and heterogeneous clouds to process, manipulate or generate data. Aiming to reduce the learning curve, as well as the operational cost of managing community specific services running on distributed cloud, DODAS completely automates the process of provisioning, creating, managing and accessing a pool of heterogeneous computing and storage resources. Within the EOSC-hub project DODAS - Thematic Service is providing both the PaaS core services and an Enabling Facility (Cloud@CNAF and Cloud@ReCaS-Bari). Although DODAS PaaS core layer can be used to exploit any cloud , we foresee that a user might benefit of a freely accessible Enabling Facility where to test a customisation and/or simply try out how DODAS behaves etc. This guide provides not only an overview of the architecture and basic concepts behind DODAS, but also a comprehensive documentation for users willing to instantiate currently supported services: HTCondor batch system as a Service Big Data platform for ML as a Service either using the DODAS provided Enabling Facility or a user provided public Cloud as well as a private Cloud (Amazon, Microsoft Azure, Open Telekom Cloud, etc). Moreover this guide provides the reader guidelines on how to customise and extend the DODAS workflow. However, if you feel you are an impatient user, this is the section you are looking for. DODAS has been integrated by the Submission Infrastructure of Compact Muon Solenoid (CMS), one of the two bigger and general purposes experiments at LHC of CERN, as well as by the Alpha Magnetic Spectrometer (AMS-02) computing environment.","title":"Introduction"},{"location":"known-issues/","text":"Known Issues Work In progress","title":"Known Issues"},{"location":"known-issues/#known-issues","text":"Work In progress","title":"Known Issues"},{"location":"monitoring/","text":"Monitoring This section provides the description of the knobs and handles DODAS provide to the end user in order to implement its own monitoring. Monitoring implementation Using DODAS monitoring backend Using third party Elasticsearch endpoint Create and deploy your own Beat or sensor Monitoring implementation DODAS deployments can be monitored sending system and docker metrics collected by a metricbeat to a desired Elasticsearch backend for monitoring both the cluster VMs and the containers running on them. Once store in Elasticsearch the information are accessible from plain REST calls or visual dashboards can be produced with dedicated tools such as Kibana or Grafana . The CMS recipe already includes the deployment of a metricbeat sensor on each Mesos slave configured as here . Two modules are used among the variety available: system and docker . Using DODAS monitoring backend It is possible to store metrics in a ready-to-use infrastructure composed by an Elasticsearch instance and an associated Grafana server where users can log in through DODAS IAM authentication. If you are interested in using these resource please contact the support team at dodas-support at lists.infn.it asking for the authorization. You will receive the credentials needed for pushing information from metricbeat to the DODAS Elasticsearch. More over a dedicate Grafana project will be created as well with you as administrator. Using third party Elasticsearch endpoint To send metrics to your own server, you can simply change the following configuration parameters within the CMS recipe monitordb_ip: type: string default: host:port elasticsearch_secret: type: string default: password N.B. default user is `dodas`, this will be configurable on next versions Create and deploy your own Beat or sensor One can create its own beat to send customized metrics to Elasticsearch following this guide . Once the beat is ready and compiled one can create a docker container with the beat binary inside to orchestrate the distribution inside the cluster as a Marathon app. If you prefer python over the golang beat approach, just create your own daemon pushing json documents in elasticsearch one by one or if you expect a big amount of docs each polling cycle push them in bulk .","title":"Monitoring"},{"location":"monitoring/#monitoring","text":"This section provides the description of the knobs and handles DODAS provide to the end user in order to implement its own monitoring. Monitoring implementation Using DODAS monitoring backend Using third party Elasticsearch endpoint Create and deploy your own Beat or sensor","title":"Monitoring"},{"location":"monitoring/#monitoring-implementation","text":"DODAS deployments can be monitored sending system and docker metrics collected by a metricbeat to a desired Elasticsearch backend for monitoring both the cluster VMs and the containers running on them. Once store in Elasticsearch the information are accessible from plain REST calls or visual dashboards can be produced with dedicated tools such as Kibana or Grafana . The CMS recipe already includes the deployment of a metricbeat sensor on each Mesos slave configured as here . Two modules are used among the variety available: system and docker .","title":"Monitoring implementation"},{"location":"monitoring/#using-dodas-monitoring-backend","text":"It is possible to store metrics in a ready-to-use infrastructure composed by an Elasticsearch instance and an associated Grafana server where users can log in through DODAS IAM authentication. If you are interested in using these resource please contact the support team at dodas-support at lists.infn.it asking for the authorization. You will receive the credentials needed for pushing information from metricbeat to the DODAS Elasticsearch. More over a dedicate Grafana project will be created as well with you as administrator.","title":"Using DODAS monitoring backend"},{"location":"monitoring/#using-third-party-elasticsearch-endpoint","text":"To send metrics to your own server, you can simply change the following configuration parameters within the CMS recipe monitordb_ip: type: string default: host:port elasticsearch_secret: type: string default: password N.B. default user is `dodas`, this will be configurable on next versions","title":"Using third party Elasticsearch endpoint"},{"location":"monitoring/#create-and-deploy-your-own-beat-or-sensor","text":"One can create its own beat to send customized metrics to Elasticsearch following this guide . Once the beat is ready and compiled one can create a docker container with the beat binary inside to orchestrate the distribution inside the cluster as a Marathon app. If you prefer python over the golang beat approach, just create your own daemon pushing json documents in elasticsearch one by one or if you expect a big amount of docs each polling cycle push them in bulk .","title":"Create and deploy your own Beat or sensor"},{"location":"the-enabling-facility/","text":"The Enabling Facility DODAS is one of the 9 Thematic Services available in the context of EOSC-hub EU project. As such, the mission of DODAS is to provide support for the integration in to the PaaS core service of use cases and workflows required by those scientific communities seeking to exploit Cloud resources to accomplish their research activities. In order to make this integration processe more concrete the DODAS team provides not only the guidance for integrating the user community workflows, but also offers the possibility to test DODAS on a freely accessible cloud. This is the so called Enabling Facility . Data and compute resources of this Enabling Facility are offered by two distinct providers at INFN: Cloud@CNAF and ReCaS@Bari . Both of them are based on Openstack middleware. The Enabling Facility is freely accessible through DODAS PaaS core services, upon successful registration and authentication on https://dodas-iam.cloud.cnaf.infn.it/ .","title":"The Enabling Facility"},{"location":"the-enabling-facility/#the-enabling-facility","text":"DODAS is one of the 9 Thematic Services available in the context of EOSC-hub EU project. As such, the mission of DODAS is to provide support for the integration in to the PaaS core service of use cases and workflows required by those scientific communities seeking to exploit Cloud resources to accomplish their research activities. In order to make this integration processe more concrete the DODAS team provides not only the guidance for integrating the user community workflows, but also offers the possibility to test DODAS on a freely accessible cloud. This is the so called Enabling Facility . Data and compute resources of this Enabling Facility are offered by two distinct providers at INFN: Cloud@CNAF and ReCaS@Bari . Both of them are based on Openstack middleware. The Enabling Facility is freely accessible through DODAS PaaS core services, upon successful registration and authentication on https://dodas-iam.cloud.cnaf.infn.it/ .","title":"The Enabling Facility"},{"location":"troubleshooting/","text":"Troubleshooting Work in progress","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"Work in progress","title":"Troubleshooting"},{"location":"getting-started/","text":"Getting Started DODAS provides two generic workflows already implemented ready to be used or extended. As example both CMS and AMS have been extended one of the two baseline implementations. In practice: DODAS provides TOSCA Templates (plus Ansible) for: HTCondor Batch System, which in turn can be: A complete and standalone HTCondor batch system (BatchSystem as a Service) as such it includes all the HTCondor services: Schedd, Central Manager and executors (startds). A HTCondor extension of a already existing Pool this is about pre configured HTCondor executors (startd) auto-join a existing HTCondor pool. BigData Platform A Machine Learning as a Service. Currently this is about a Spark Framework which can be coupled with a HDFS (either pre-existing or generated on demand) for data ingestion.","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"DODAS provides two generic workflows already implemented ready to be used or extended. As example both CMS and AMS have been extended one of the two baseline implementations. In practice: DODAS provides TOSCA Templates (plus Ansible) for: HTCondor Batch System, which in turn can be: A complete and standalone HTCondor batch system (BatchSystem as a Service) as such it includes all the HTCondor services: Schedd, Central Manager and executors (startds). A HTCondor extension of a already existing Pool this is about pre configured HTCondor executors (startd) auto-join a existing HTCondor pool. BigData Platform A Machine Learning as a Service. Currently this is about a Spark Framework which can be coupled with a HDFS (either pre-existing or generated on demand) for data ingestion.","title":"Getting Started"},{"location":"getting-started/ams-recipe/","text":"HTCondor AMS Recipe Work In Progress","title":"HTCondor AMS Recipe"},{"location":"getting-started/ams-recipe/#htcondor-ams-recipe","text":"Work In Progress","title":"HTCondor AMS Recipe"},{"location":"getting-started/cms-recipe/","text":"HTCondor CMS Recipe The DODAS workflow implemented for CMS has been designed in order to generate an ephemeral Tier* WLCG compliant. Prerequisites In the basic implementation has been built on the following assumptions There is no Computing Element . Worker nodes (HTCondor startd processes) start up as a docker container over Mesos cluster, and auto-join the HTCondor Global-Pool of CMS Data I/O is meant to rely on AAA xrootd read rule although there is not technical limitation preventing the usage of local storages. stage-out relies on a Tier site of CMS, e.g. INFN relies on TI_IT_CNAF. The result is something like this in the site local config text url=\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/T1_IT_CNAF/PhEDEx/storage.xml?protocol=srmv2\"/ This imply to accomplish with the following pre-requisites: Requires Submission Infrastructure (SI) L2s authorization for Global-Pool access. In order to being authorized you must belong to the CMS Collaboration and provide a DN and CMS Site Name. SI will use these info to define the proper mapping in the match-maker. Get a DN from X.509 Certificate you can retrieve from the Token Translation Service . 1. Click to request a x509. 2. A pop-up window will allow you to download the certificate PEM file. At that point you should run openssl x509 -noout -in certificate.pem -subject 3. and you will obtain something like subject= /C=IT/O=CLOUD@CNAF/CN=xxxxxxx@dodas-iam Define a name for your ephemeral CMS Site: e.g. T3_XX_Opportunistic_KK If you want to be visible in the Dashboard (this is ONLY true for the old-fashioned Dashboard ) you need to notify the dashboard support team informing that you need the following mapping among Site Name and SyncCE Site Name == T3_XX_Opportunistic_KK SyncCE == T3_XX_Opportunistic_KK NOTE : This is needed because DODAS does not deploy a cluster which relies on a Computing Element. You need to provide a job id to the CERN monitoring team. Long Running Services Once done all of this, you should be able to get this TOSCA template and configure everything as described in the template itself. The CMS template deploys the following services and components: - squid proxy - proxy cache - worker node (HTCondor startd) - cvmfs - cvmfs-check app - CMS Trivial File Catalogue Docker image files are available here . Launching a DODAS instance of HTCondor for CMS This assume you are now familiar with following steps: how to GET a token from IAM-DODAS how to submit a TOSCA template (either with PaaS orchestrator or Infrastructure Manager) You can get the basic CMS TOSCA template from here and submit it after a proper configuration. There input parameters to be set are explained below. THere are 3 sections and these are the mandatory parameters. For advanced usage there is more to be configured. Marathon and Mesos related configuration parameters marathon_username : Admin username for Marathon GUI marathon_password : Admin password for for Marathon GUI mesos_username : Admin username for Mesos GUI mesos_password : Admin password for Mesos GUI number_of_masters : num_cpus_master : mem_size_master : number_of_slaves : num_cpus_slave : mem_size_slave: number_of_lbs : num_cpus_lb : mem_size_lb : server_image : Image for the Virtual Machine to be used. NOTE all the recipes are validated for Ubuntu Xenial. IAM related configurations to enable the OIDC to X.509 certificate translation iam_token : The token string obtained as explained here Iam_client_id : This must be provided (once) by DODAS admins iam_client_secret : This must be provided (once) by DODAS admins CMS specific configurations cms_local_site : This is a name of the format T3_XX_Opportunistic_KK. You decide this as explained here . cms_stageoutsite : This must be either an already existing T1/2/3, or a new site you will register in the CMS computing system. cms_stageoutprotocol : this is the protocol you want to use, to be set accordingly with one of the options supported by the SITECONF related to the cms_stageoutsite. Once the cluster has been created you should be able to access the Marathon and Mesos GUIs for management, debugging etc. The very last step of the deployment is the start-up HTCondor startd process. If no errors are encountered the startds should join the HTCondor global pool automatically, and thus if matching happens HTCondor start executing payloads. At that point, most probably, you would like to submit some jobs with proper configuration to allow the matching. Submitting CRAB jobs for DODAS CMS Site In order to submit CRAB jobs with proper classad parameters which guarantee the matching, you need to add this extra line in the configuration file of CRAB: config.Debug.extraJDL = [ '+DESIRED_Sites= T3_XX_XY_KK ','+JOB_CMSSite= T3_XX_XY_KK ','+AccountingGroup= highprio. YOUR_LXPLUS_LOGIN ' ] There is no any other change you need to do. Finally there is a basic Elastic Search monitoring system which can be used and extended to cope with user specific needs. This is detailed here .","title":"HTCondor CMS Recipe"},{"location":"getting-started/cms-recipe/#htcondor-cms-recipe","text":"The DODAS workflow implemented for CMS has been designed in order to generate an ephemeral Tier* WLCG compliant.","title":"HTCondor CMS Recipe"},{"location":"getting-started/cms-recipe/#prerequisites","text":"In the basic implementation has been built on the following assumptions There is no Computing Element . Worker nodes (HTCondor startd processes) start up as a docker container over Mesos cluster, and auto-join the HTCondor Global-Pool of CMS Data I/O is meant to rely on AAA xrootd read rule although there is not technical limitation preventing the usage of local storages. stage-out relies on a Tier site of CMS, e.g. INFN relies on TI_IT_CNAF. The result is something like this in the site local config text url=\"trivialcatalog_file:/cvmfs/cms.cern.ch/SITECONF/T1_IT_CNAF/PhEDEx/storage.xml?protocol=srmv2\"/ This imply to accomplish with the following pre-requisites: Requires Submission Infrastructure (SI) L2s authorization for Global-Pool access. In order to being authorized you must belong to the CMS Collaboration and provide a DN and CMS Site Name. SI will use these info to define the proper mapping in the match-maker. Get a DN from X.509 Certificate you can retrieve from the Token Translation Service . 1. Click to request a x509. 2. A pop-up window will allow you to download the certificate PEM file. At that point you should run openssl x509 -noout -in certificate.pem -subject 3. and you will obtain something like subject= /C=IT/O=CLOUD@CNAF/CN=xxxxxxx@dodas-iam Define a name for your ephemeral CMS Site: e.g. T3_XX_Opportunistic_KK If you want to be visible in the Dashboard (this is ONLY true for the old-fashioned Dashboard ) you need to notify the dashboard support team informing that you need the following mapping among Site Name and SyncCE Site Name == T3_XX_Opportunistic_KK SyncCE == T3_XX_Opportunistic_KK NOTE : This is needed because DODAS does not deploy a cluster which relies on a Computing Element. You need to provide a job id to the CERN monitoring team.","title":"Prerequisites"},{"location":"getting-started/cms-recipe/#long-running-services","text":"Once done all of this, you should be able to get this TOSCA template and configure everything as described in the template itself. The CMS template deploys the following services and components: - squid proxy - proxy cache - worker node (HTCondor startd) - cvmfs - cvmfs-check app - CMS Trivial File Catalogue Docker image files are available here .","title":"Long Running Services"},{"location":"getting-started/cms-recipe/#launching-a-dodas-instance-of-htcondor-for-cms","text":"This assume you are now familiar with following steps: how to GET a token from IAM-DODAS how to submit a TOSCA template (either with PaaS orchestrator or Infrastructure Manager) You can get the basic CMS TOSCA template from here and submit it after a proper configuration. There input parameters to be set are explained below. THere are 3 sections and these are the mandatory parameters. For advanced usage there is more to be configured. Marathon and Mesos related configuration parameters marathon_username : Admin username for Marathon GUI marathon_password : Admin password for for Marathon GUI mesos_username : Admin username for Mesos GUI mesos_password : Admin password for Mesos GUI number_of_masters : num_cpus_master : mem_size_master : number_of_slaves : num_cpus_slave : mem_size_slave: number_of_lbs : num_cpus_lb : mem_size_lb : server_image : Image for the Virtual Machine to be used. NOTE all the recipes are validated for Ubuntu Xenial. IAM related configurations to enable the OIDC to X.509 certificate translation iam_token : The token string obtained as explained here Iam_client_id : This must be provided (once) by DODAS admins iam_client_secret : This must be provided (once) by DODAS admins CMS specific configurations cms_local_site : This is a name of the format T3_XX_Opportunistic_KK. You decide this as explained here . cms_stageoutsite : This must be either an already existing T1/2/3, or a new site you will register in the CMS computing system. cms_stageoutprotocol : this is the protocol you want to use, to be set accordingly with one of the options supported by the SITECONF related to the cms_stageoutsite. Once the cluster has been created you should be able to access the Marathon and Mesos GUIs for management, debugging etc. The very last step of the deployment is the start-up HTCondor startd process. If no errors are encountered the startds should join the HTCondor global pool automatically, and thus if matching happens HTCondor start executing payloads. At that point, most probably, you would like to submit some jobs with proper configuration to allow the matching.","title":"Launching a DODAS instance of HTCondor for CMS"},{"location":"getting-started/cms-recipe/#submitting-crab-jobs-for-dodas-cms-site","text":"In order to submit CRAB jobs with proper classad parameters which guarantee the matching, you need to add this extra line in the configuration file of CRAB: config.Debug.extraJDL = [ '+DESIRED_Sites= T3_XX_XY_KK ','+JOB_CMSSite= T3_XX_XY_KK ','+AccountingGroup= highprio. YOUR_LXPLUS_LOGIN ' ] There is no any other change you need to do. Finally there is a basic Elastic Search monitoring system which can be used and extended to cope with user specific needs. This is detailed here .","title":"Submitting CRAB jobs for DODAS CMS Site"},{"location":"getting-started/recipe-for-impatient-users/","text":"Recipe for impatient users An impatient user seeking to try a DODAS deployment need to address the following 4 main steps: 1) Registration Register to the IAM-DODAS service by accessing the service here . You can use your IdP because IAM-DODAS supports eduGAIN identity federation. The first registration will require the approval from the DODAS admins. 2) Token Management Once your registration has been approved you can get your first DODAS token by using the recipe described and detailed here . As you can see there are two options currently supported although we consider the password flow deprecated. We strongly suggest the Device code Flow for all the reasons detailed. Note that for the Device code flow, you need to configure the following in the Client (self-generated) management dashboard : Access - grant_types - token . For a very impatient user: just download and execute this script . Please note The script requires that you have a client (authorized for Device-Flow). You can either have your client (self-service generated) or use a client provided by DODAS team. In both cases before running the script you need to know the following information: text IAM_DEVICE_CODE_CLIENT_ID IAM_DEVICE_CODE_CLIENT_SECRET IAM_DEVICE_CODE_ENDPOINT IAM_TOKEN_ENDPOINT There will be a few steps to address. The script will guide you. You can set IAM_DEVICE_CODE_ENDPOINT=\" https://dodas-iam.cloud.cnaf.infn.it/devicecode \" and IAM_TOKEN_ENDPOINT=\" https://dodas-iam.cloud.cnaf.infn.it/token \" in the script. Client ID and secret are given when you create the Device code flow client as explained above . There are two kinds of token: access token : short-lived. This is used in the steps below to obtain resources. refresh token : long-lived. Used to refresh the access token without going through device authorization in browser. See for example this script . 3) Prepare your TOSCA template At that point you can checkout the already available TOSCA Templates here and pick the one you prefer. Otherwise just use the following simple TOSCA test to get a taste of the whole system (just replace os:properties:image: Ubuntu_16.04 with an existing image): imports: - indigo_custom_types: https://raw.githubusercontent.com/indigo-dc/tosca-types/master/custom_types.yaml description: Launch a VM Get IP and SSH credentials to access topology_template: node_templates: simple_node: type: tosca.nodes.indigo.Compute capabilities: endpoint: properties: network_name: PUBLIC ports: user_port: protocol: tcp source: 9000 other_port: protocol: tcp source: 9001 scalable: properties: count: 1 host: properties: instance_type: m1.small os: properties: image: Ubuntu_16.04 outputs: node_ip: value: { get_attribute: [ simple_node, public_address, 0 ] } node_creds: value: { get_attribute: [ simple_node, endpoint, credential, 0 ] } If you will choose a specific template (not just the test once) you need to properly configure it. Configuration parameters should be documented on each TOSCA template. 4) Submit the TOSCA template Once configured, you can submit the TOSCA either to the PaaS Orchestrator or directly to IM. As a start, and if no complicated cloud configurations are needed, we recommend to start with direct submission to IM. Direct submission to IM The direct submission to IM can be done either via im-client or using the RESTful API. The extended guide provides all the recipes and information for installation and configuration of the CLI as well as the documentation of the REST APIs. However using the REST is highly suggested for the initial testing cause it guarantees a enormous flexibility, useful for a fast turnaround during tests. In any case the public endpoint is : https://im.cloud.cnaf.infn.it:8800/infrastructures And an example of REST based submission is curl -k -H 'Content-type: text/yaml' -H Authorization: id = ost; type = OpenStack; host = https://horizon.cloud.cnaf.infn.it:5000/v3; username = indigo-dc; password = $IAM_ACCESS_TOKEN; tenant = oidc; auth_version = 3.x_oidc_access_token; service_region = regionOne;\\nid = im; type = InfrastructureManager; token = $IAM_ACCESS_TOKEN -X POST http://im.cloud.cnaf.infn.it:8800/infrastructures --data-binary @ YOUR_TOSCA .yaml And the expected output should be something like: HTTP/1.1 200 OK Content-Length: 86 Content-Type: text/uri-list Infid: 9b044cce-6424-11e8-bad9-0242ac120003 Date: Wed, 30 May 2018 16:15:11 GMT Server: Cheroot/6.3.1 http://im.cloud.cnaf.infn.it:8800/infrastructures/9b044cce-6424-11e8-bad9-0242ac120003 To submit to BARI, please use the following curl: curl -v -k -H 'Content-type: text/yaml' -H Authorization: id = os; type = OpenStack; host = https://cloud.recas.ba.infn.it:5000/; username = indigo-dc; password = $IAM_ACCESS_TOKEN; tenant = oidc; auth_version = 3.x_oidc_access_token; service_region = recas-cloud;\\nid = im; type = InfrastructureManager; token = $IAM_ACCESS_TOKEN -i -X POST https://im.cloud.cnaf.infn.it:443/infrastructures --data-binary @tosca-templates/dodas/CMS-HTCondor-dodas.yaml The above commands are based on POST (to create the infrastructure described in the TOSCA template), but you can also use GET (to list) or DELETE commands to manage the infrastructure that you have created. Please refer to the documentation linked here . Submission to the PaaS Orchestrator The Submission to the PaaS Orchestrator as well can be done both through the client and through the REST APIS. Here in this guide only the client based solution is taken into account. There are two steps: Installation the client called orchent following the recipe here . Once installed... Configuration and usage of the orchent client as described here . Be careful to the following notes: Despite the possibility to use the oidc-client we suggest to the ORCHENT_TOKEN based solution as described in the guide. Set the ORCHENT_URL env. variable using the endpoint as here below: text export ORCHENT_URL=https://orchestrator.cloud.cnaf.infn.it/orchestrator The submission command should look like the following text orchent depcreate Your-TOSCA .yaml '{}' the parenthesis '{}' can be used to pass the input parameter to the TOSCA. Although values can be filled in the template itself, the parenthesis must be left there otherwise you'll get an error. The output of the deployment creation ( depcreate ) command will be something like the following Deployment [b8bdccf3-9be5-499f-aac2-664dc0726795]: status: CREATE_IN_PROGRESS creation time: 2018-06-16T15:58+0000 update time: 2018-06-16T15:58+0000 callback: status reason: task: NONE CloudProviderName: outputs: {} links: self [https://orchestrator.cloud.cnaf.infn.it/orchestrator/deployments/b8bdccf3-9be5-499f-aac2-664dc0726795] resources [https://orchestrator.cloud.cnaf.infn.it/orchestrator/deployments/b8bdccf3-9be5-499f-aac2-664dc0726795/resources] template [https://orchestrator.cloud.cnaf.infn.it/orchestrator/deployments/b8bdccf3-9be5-499f-aac2-664dc0726795/template] The above steps 1 to 3 are valid irrespective of which TOSCA template will be used. Templates available are HTCondor as batch system and Spark. Moreover there are Experiment specific customization, in particular CMS and AMS recipes. User can test all the recipes running on the freely accessible DODAS Enabling Facility .","title":"Recipe for impatient users"},{"location":"getting-started/recipe-for-impatient-users/#recipe-for-impatient-users","text":"An impatient user seeking to try a DODAS deployment need to address the following 4 main steps:","title":"Recipe for impatient users"},{"location":"getting-started/recipe-for-impatient-users/#141-registration","text":"Register to the IAM-DODAS service by accessing the service here . You can use your IdP because IAM-DODAS supports eduGAIN identity federation. The first registration will require the approval from the DODAS admins.","title":"1) Registration"},{"location":"getting-started/recipe-for-impatient-users/#241-token-management","text":"Once your registration has been approved you can get your first DODAS token by using the recipe described and detailed here . As you can see there are two options currently supported although we consider the password flow deprecated. We strongly suggest the Device code Flow for all the reasons detailed. Note that for the Device code flow, you need to configure the following in the Client (self-generated) management dashboard : Access - grant_types - token . For a very impatient user: just download and execute this script . Please note The script requires that you have a client (authorized for Device-Flow). You can either have your client (self-service generated) or use a client provided by DODAS team. In both cases before running the script you need to know the following information: text IAM_DEVICE_CODE_CLIENT_ID IAM_DEVICE_CODE_CLIENT_SECRET IAM_DEVICE_CODE_ENDPOINT IAM_TOKEN_ENDPOINT There will be a few steps to address. The script will guide you. You can set IAM_DEVICE_CODE_ENDPOINT=\" https://dodas-iam.cloud.cnaf.infn.it/devicecode \" and IAM_TOKEN_ENDPOINT=\" https://dodas-iam.cloud.cnaf.infn.it/token \" in the script. Client ID and secret are given when you create the Device code flow client as explained above . There are two kinds of token: access token : short-lived. This is used in the steps below to obtain resources. refresh token : long-lived. Used to refresh the access token without going through device authorization in browser. See for example this script .","title":"2) Token Management"},{"location":"getting-started/recipe-for-impatient-users/#341-prepare-your-tosca-template","text":"At that point you can checkout the already available TOSCA Templates here and pick the one you prefer. Otherwise just use the following simple TOSCA test to get a taste of the whole system (just replace os:properties:image: Ubuntu_16.04 with an existing image): imports: - indigo_custom_types: https://raw.githubusercontent.com/indigo-dc/tosca-types/master/custom_types.yaml description: Launch a VM Get IP and SSH credentials to access topology_template: node_templates: simple_node: type: tosca.nodes.indigo.Compute capabilities: endpoint: properties: network_name: PUBLIC ports: user_port: protocol: tcp source: 9000 other_port: protocol: tcp source: 9001 scalable: properties: count: 1 host: properties: instance_type: m1.small os: properties: image: Ubuntu_16.04 outputs: node_ip: value: { get_attribute: [ simple_node, public_address, 0 ] } node_creds: value: { get_attribute: [ simple_node, endpoint, credential, 0 ] } If you will choose a specific template (not just the test once) you need to properly configure it. Configuration parameters should be documented on each TOSCA template.","title":"3) Prepare your TOSCA template"},{"location":"getting-started/recipe-for-impatient-users/#441-submit-the-tosca-template","text":"Once configured, you can submit the TOSCA either to the PaaS Orchestrator or directly to IM. As a start, and if no complicated cloud configurations are needed, we recommend to start with direct submission to IM.","title":"4) Submit the TOSCA template"},{"location":"getting-started/recipe-for-impatient-users/#direct-submission-to-im","text":"The direct submission to IM can be done either via im-client or using the RESTful API. The extended guide provides all the recipes and information for installation and configuration of the CLI as well as the documentation of the REST APIs. However using the REST is highly suggested for the initial testing cause it guarantees a enormous flexibility, useful for a fast turnaround during tests. In any case the public endpoint is : https://im.cloud.cnaf.infn.it:8800/infrastructures And an example of REST based submission is curl -k -H 'Content-type: text/yaml' -H Authorization: id = ost; type = OpenStack; host = https://horizon.cloud.cnaf.infn.it:5000/v3; username = indigo-dc; password = $IAM_ACCESS_TOKEN; tenant = oidc; auth_version = 3.x_oidc_access_token; service_region = regionOne;\\nid = im; type = InfrastructureManager; token = $IAM_ACCESS_TOKEN -X POST http://im.cloud.cnaf.infn.it:8800/infrastructures --data-binary @ YOUR_TOSCA .yaml And the expected output should be something like: HTTP/1.1 200 OK Content-Length: 86 Content-Type: text/uri-list Infid: 9b044cce-6424-11e8-bad9-0242ac120003 Date: Wed, 30 May 2018 16:15:11 GMT Server: Cheroot/6.3.1 http://im.cloud.cnaf.infn.it:8800/infrastructures/9b044cce-6424-11e8-bad9-0242ac120003 To submit to BARI, please use the following curl: curl -v -k -H 'Content-type: text/yaml' -H Authorization: id = os; type = OpenStack; host = https://cloud.recas.ba.infn.it:5000/; username = indigo-dc; password = $IAM_ACCESS_TOKEN; tenant = oidc; auth_version = 3.x_oidc_access_token; service_region = recas-cloud;\\nid = im; type = InfrastructureManager; token = $IAM_ACCESS_TOKEN -i -X POST https://im.cloud.cnaf.infn.it:443/infrastructures --data-binary @tosca-templates/dodas/CMS-HTCondor-dodas.yaml The above commands are based on POST (to create the infrastructure described in the TOSCA template), but you can also use GET (to list) or DELETE commands to manage the infrastructure that you have created. Please refer to the documentation linked here .","title":"Direct submission to IM"},{"location":"getting-started/recipe-for-impatient-users/#submission-to-the-paas-orchestrator","text":"The Submission to the PaaS Orchestrator as well can be done both through the client and through the REST APIS. Here in this guide only the client based solution is taken into account. There are two steps: Installation the client called orchent following the recipe here . Once installed... Configuration and usage of the orchent client as described here . Be careful to the following notes: Despite the possibility to use the oidc-client we suggest to the ORCHENT_TOKEN based solution as described in the guide. Set the ORCHENT_URL env. variable using the endpoint as here below: text export ORCHENT_URL=https://orchestrator.cloud.cnaf.infn.it/orchestrator The submission command should look like the following text orchent depcreate Your-TOSCA .yaml '{}' the parenthesis '{}' can be used to pass the input parameter to the TOSCA. Although values can be filled in the template itself, the parenthesis must be left there otherwise you'll get an error. The output of the deployment creation ( depcreate ) command will be something like the following Deployment [b8bdccf3-9be5-499f-aac2-664dc0726795]: status: CREATE_IN_PROGRESS creation time: 2018-06-16T15:58+0000 update time: 2018-06-16T15:58+0000 callback: status reason: task: NONE CloudProviderName: outputs: {} links: self [https://orchestrator.cloud.cnaf.infn.it/orchestrator/deployments/b8bdccf3-9be5-499f-aac2-664dc0726795] resources [https://orchestrator.cloud.cnaf.infn.it/orchestrator/deployments/b8bdccf3-9be5-499f-aac2-664dc0726795/resources] template [https://orchestrator.cloud.cnaf.infn.it/orchestrator/deployments/b8bdccf3-9be5-499f-aac2-664dc0726795/template] The above steps 1 to 3 are valid irrespective of which TOSCA template will be used. Templates available are HTCondor as batch system and Spark. Moreover there are Experiment specific customization, in particular CMS and AMS recipes. User can test all the recipes running on the freely accessible DODAS Enabling Facility .","title":"Submission to the PaaS Orchestrator"},{"location":"using-dodas-with-external-providers/","text":"Using DODAS with external providers In order to use DODAS core-PaaS Services to deploy clusters on your preferred IaaS there are two options one through the PaaS Orchestrator and one using a direct interaction with IM. Whether to use option one or two is actually strongly related to the scenario you need to address. The very basic scenario with just one Cloud provider can be addressed with direct IM interaction, everything else, spanning from a multy-hibrid cloud management up to the elastic extension, requires the PaaS Orchestrator. The usage of PaaS Orchestrator with external IaaS (wrt DODAS provided one) requires: A pre-registration of the actual IaaS provider A policy definition in the TOSCA template for credential management This became optional depending whether the IaaS is granting a DODAS-IAM federated authentication or not. Federation could not be possible at all in public Clouds","title":"Using DODAS with external providers"},{"location":"using-dodas-with-external-providers/#using-dodas-with-external-providers","text":"In order to use DODAS core-PaaS Services to deploy clusters on your preferred IaaS there are two options one through the PaaS Orchestrator and one using a direct interaction with IM. Whether to use option one or two is actually strongly related to the scenario you need to address. The very basic scenario with just one Cloud provider can be addressed with direct IM interaction, everything else, spanning from a multy-hibrid cloud management up to the elastic extension, requires the PaaS Orchestrator. The usage of PaaS Orchestrator with external IaaS (wrt DODAS provided one) requires: A pre-registration of the actual IaaS provider A policy definition in the TOSCA template for credential management This became optional depending whether the IaaS is granting a DODAS-IAM federated authentication or not. Federation could not be possible at all in public Clouds","title":"Using DODAS with external providers"},{"location":"using-dodas-with-external-providers/how-to-use-a-public-cloud/","text":"How to use a public IaaS with DODAS-PaaS Work in progress","title":"How to use a public IaaS with DODAS-PaaS"},{"location":"using-dodas-with-external-providers/how-to-use-a-public-cloud/#how-to-use-a-public-iaas-with-dodas-paas","text":"Work in progress","title":"How to use a public IaaS with DODAS-PaaS"},{"location":"using-dodas-with-external-providers/how-to-use-your-own-cloud/","text":"How to use private IaaS with DODAS-PaaS Work in Progress","title":"How to use private IaaS with DODAS-PaaS"},{"location":"using-dodas-with-external-providers/how-to-use-your-own-cloud/#how-to-use-private-iaas-with-dodas-paas","text":"Work in Progress","title":"How to use private IaaS with DODAS-PaaS"}]}